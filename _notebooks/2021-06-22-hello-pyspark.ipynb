{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayurjainf007/PySpark_Interview_Questions/blob/notebook/_notebooks/2021-06-22-hello-pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOYl-JdJxB3N",
        "outputId": "2b99d9d6-a279-492e-9ea4-712ad9048199"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.functions import col,split\n",
        "\n",
        "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "StructField(\"Emp_ID\", IntegerType(), True),\n",
        "StructField(\"Name\", StringType(), True),\n",
        "StructField(\"Age\", IntegerType(), True),\n",
        "StructField(\"Salary\", IntegerType(), True),\n",
        "StructField(\"Department\", StringType(), True),\n",
        "StructField(\"Address\", StringType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "data = [ (101, \"Rajesh\", 30, 60000, \"IT\", \"Mumbai, Maharashtra\"),\n",
        "(102, \"Priya\", 28, 75000, \"HR\", \"Bengaluru, Karnataka\"),\n",
        "(103, \"Suresh\", 35, 50000, \"Finance\", \"Chennai, Tamil Nadu\"),\n",
        "(104, \"Anjali\", 25, 80000, \"IT\", \"Pune, Maharashtra\"), (105, \"Arjun\", 40, 90000, \"Management\", \"Hyderabad, Telangana\") ]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "display(df)\n",
        "\n",
        "filtered_df = df.filter((col('Department')=='IT') & (col('Salary') > 70000))\n",
        "df_transformed = filtered_df.withColumn('City', split(col('Address'),',')[0]) \\\n",
        "                            .withColumn('State', split(col('Address'),',')[1]) \\\n",
        "                            .drop('Address')\n",
        "df_transformed.show()\n",
        "df_transformed.write.mode('overwrite').parquet('output.parquet')\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "pweXdSpUw_kM",
        "outputId": "c32ce113-fb3d-414a-ff77-bd40fb6b738a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Emp_ID: int, Name: string, Age: int, Salary: int, Department: string, Address: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+---+------+----------+----+------------+\n",
            "|Emp_ID|  Name|Age|Salary|Department|City|       State|\n",
            "+------+------+---+------+----------+----+------------+\n",
            "|   104|Anjali| 25| 80000|        IT|Pune| Maharashtra|\n",
            "+------+------+---+------+----------+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, rank, avg\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WindowFunctionExample\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"HR\", 5000),\n",
        "    (2, \"HR\", 6000),\n",
        "    (3, \"HR\", 4500),\n",
        "    (4, \"IT\", 7000),\n",
        "    (5, \"IT\", 7200),\n",
        "    (6, \"IT\", 7100),\n",
        "    (7, \"Finance\", 6500),\n",
        "    (8, \"Finance\", 6300),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"employee_id\", \"department\", \"salary\"])\n",
        "\n",
        "rank_window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
        "df_with_rank = df.withColumn(\"rank\", rank().over(rank_window))\n",
        "df_with_rank.show()\n",
        "\n",
        "avg_salary_window = Window.partitionBy(\"department\")\n",
        "df_with_avg_salary = df.withColumn(\"avg_salary\", avg(col(\"salary\")).over(avg_salary_window))\n",
        "df_with_avg_salary.show()\n",
        "\n",
        "top_2_employees = df_with_rank.filter(col(\"rank\") <= 2)\n",
        "top_2_employees.show()\n",
        "\n",
        "top_2_employees_by_dept = df_with_rank.filter(col(\"rank\") <= 2).orderBy(col(\"department\"), col(\"rank\"))\n",
        "top_2_employees_by_dept.show()\n",
        "\n",
        "top_2_emplyoees_by_avg_salary = df_with_avg_salary.filter(col(\"salary\") >= col(\"avg_salary\"))\n",
        "top_2_emplyoees_by_avg_salary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXgC0EPZx-lQ",
        "outputId": "a742a4a8-f5d3-45a3-b985-461a57592572"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------+----+\n",
            "|employee_id|department|salary|rank|\n",
            "+-----------+----------+------+----+\n",
            "|          7|   Finance|  6500|   1|\n",
            "|          8|   Finance|  6300|   2|\n",
            "|          2|        HR|  6000|   1|\n",
            "|          1|        HR|  5000|   2|\n",
            "|          3|        HR|  4500|   3|\n",
            "|          5|        IT|  7200|   1|\n",
            "|          6|        IT|  7100|   2|\n",
            "|          4|        IT|  7000|   3|\n",
            "+-----------+----------+------+----+\n",
            "\n",
            "+-----------+----------+------+-----------------+\n",
            "|employee_id|department|salary|       avg_salary|\n",
            "+-----------+----------+------+-----------------+\n",
            "|          7|   Finance|  6500|           6400.0|\n",
            "|          8|   Finance|  6300|           6400.0|\n",
            "|          1|        HR|  5000|5166.666666666667|\n",
            "|          2|        HR|  6000|5166.666666666667|\n",
            "|          3|        HR|  4500|5166.666666666667|\n",
            "|          4|        IT|  7000|           7100.0|\n",
            "|          5|        IT|  7200|           7100.0|\n",
            "|          6|        IT|  7100|           7100.0|\n",
            "+-----------+----------+------+-----------------+\n",
            "\n",
            "+-----------+----------+------+----+\n",
            "|employee_id|department|salary|rank|\n",
            "+-----------+----------+------+----+\n",
            "|          7|   Finance|  6500|   1|\n",
            "|          8|   Finance|  6300|   2|\n",
            "|          2|        HR|  6000|   1|\n",
            "|          1|        HR|  5000|   2|\n",
            "|          5|        IT|  7200|   1|\n",
            "|          6|        IT|  7100|   2|\n",
            "+-----------+----------+------+----+\n",
            "\n",
            "+-----------+----------+------+----+\n",
            "|employee_id|department|salary|rank|\n",
            "+-----------+----------+------+----+\n",
            "|          7|   Finance|  6500|   1|\n",
            "|          8|   Finance|  6300|   2|\n",
            "|          2|        HR|  6000|   1|\n",
            "|          1|        HR|  5000|   2|\n",
            "|          5|        IT|  7200|   1|\n",
            "|          6|        IT|  7100|   2|\n",
            "+-----------+----------+------+----+\n",
            "\n",
            "+-----------+----------+------+-----------------+\n",
            "|employee_id|department|salary|       avg_salary|\n",
            "+-----------+----------+------+-----------------+\n",
            "|          7|   Finance|  6500|           6400.0|\n",
            "|          2|        HR|  6000|5166.666666666667|\n",
            "|          5|        IT|  7200|           7100.0|\n",
            "|          6|        IT|  7100|           7100.0|\n",
            "+-----------+----------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_window = df.withColumn(\"rank\", rank().over(rank_window))\\\n",
        "                    .withColumn(\"avg_salary\", avg(col(\"salary\")).over(avg_salary_window))\n",
        "top_employees = df_with_window.filter(col(\"rank\") <= 2)\n",
        "top_employees.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMmp0TUx2Lnk",
        "outputId": "0c240318-95e6-4b84-b9e7-587a62f96512"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------+----+-----------------+\n",
            "|employee_id|department|salary|rank|       avg_salary|\n",
            "+-----------+----------+------+----+-----------------+\n",
            "|          7|   Finance|  6500|   1|           6400.0|\n",
            "|          8|   Finance|  6300|   2|           6400.0|\n",
            "|          2|        HR|  6000|   1|5166.666666666667|\n",
            "|          1|        HR|  5000|   2|5166.666666666667|\n",
            "|          5|        IT|  7200|   1|           7100.0|\n",
            "|          6|        IT|  7100|   2|           7100.0|\n",
            "+-----------+----------+------+----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids = [101, 102, 103, 104, 105]\n",
        "# Your code here\n",
        "even_square_id = {id: id**2 for id in user_ids if id%2==0}\n",
        "print(even_square_id)\n"
      ],
      "metadata": {
        "id": "zKMSxHZ3-CWX",
        "outputId": "58a7275f-fa7e-4cce-bd0f-959a44d62ccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{102: 10404, 104: 10816}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "nums = [1, 2, 3, 4]\n",
        "doubled = list(map(lambda x: x * 2, nums))         # [2, 4, 6, 8]\n",
        "evens = list(filter(lambda x: x % 2 == 0, nums))   # [2, 4]\n",
        "product = reduce(lambda x, y: x * y, nums)         # 24"
      ],
      "metadata": {
        "id": "eBfXSpHH-1pa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def even_generator(n):\n",
        "  for i in range(n):\n",
        "    if i%2 == 0:\n",
        "      yield i\n",
        "\n",
        "gen = even_generator(20)\n",
        "for num in gen:\n",
        "    print(num)"
      ],
      "metadata": {
        "id": "qY44pprL-YnL",
        "outputId": "623a70de-4fc2-4f15-a480-5c5eeeec7dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def divide(a,b):\n",
        "  try:\n",
        "    res = a/b\n",
        "  except ZeroDivisionError:\n",
        "    print('Not divisible by zero')\n",
        "  else:\n",
        "    print(res)\n",
        "  finally:\n",
        "    print('Done')\n",
        "\n",
        "divide(256,4)\n",
        "divide(12,0)"
      ],
      "metadata": {
        "id": "9-cXFgvE_DSQ",
        "outputId": "358cb26c-6aba-472e-be80-65602425bcd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.0\n",
            "Done\n",
            "Not divisible by zero\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"User123 logged in at 10:23AM\"\n",
        "match = re.search(r'\\d{2}:\\d{2}[AP]M', text)\n",
        "print(match.group())  # 10:23AM\n",
        "\n",
        "pattern = re.compile(r'\\d+')\n",
        "print(pattern.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping'))\n"
      ],
      "metadata": {
        "id": "yhGe03Lf_v5L",
        "outputId": "7becd4cb-308d-4aa5-a20b-2cea6d0ddc9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10:23AM\n",
            "['12', '11', '10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Reach out to us at support@example.com or careers@meta.com.\"\n",
        "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "print(emails)"
      ],
      "metadata": {
        "id": "yx8IP_TNAD6h",
        "outputId": "339ec16b-21aa-4309-e258-502c3fef12e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@example.com', 'careers@meta.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.functions import col, split, from_unixtime\n",
        "\n",
        "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
        "\n",
        "data = [ (1, \"2025-01-31 08:00:00\", \"2025-01-31 10:30:45\"),\n",
        "(2, \"2025-01-31 09:00:30\", \"2025-01-31 12:15:10\"),\n",
        "(3, \"2025-01-31 07:45:00\", \"2025-01-31 09:00:15\") ]\n",
        "\n",
        "schema = [\"user_id\", \"login_timestamp\", \"logout_timestamp\"]\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "\n",
        "df = df.withColumn('login_timestamp', col('login_timestamp').cast('timestamp'))\n",
        "df = df.withColumn('logout_timestamp', col('logout_timestamp').cast('timestamp'))\n",
        "df = df.withColumn('duration_seconds', col('logout_timestamp').cast('long') - col('login_timestamp').cast('long'))\n",
        "# in HH:mm:ss\n",
        "df = df.withColumn('duration_hms', from_unixtime(col('duration_seconds'), 'HH:mm:ss'))\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9-CgmHcDdDJ",
        "outputId": "9f83500f-45cc-46e2-8010-5ed150461ea0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+------------+\n",
            "|user_id|    login_timestamp|   logout_timestamp|duration_seconds|duration_hms|\n",
            "+-------+-------------------+-------------------+----------------+------------+\n",
            "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|            9045|    02:30:45|\n",
            "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|           11680|    03:14:40|\n",
            "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|            4515|    01:15:15|\n",
            "+-------+-------------------+-------------------+----------------+------------+\n",
            "\n",
            "root\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- login_timestamp: timestamp (nullable = true)\n",
            " |-- logout_timestamp: timestamp (nullable = true)\n",
            " |-- duration_seconds: long (nullable = true)\n",
            " |-- duration_hms: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðð®ðžð¬ð­ð¢ð¨ð§:\n",
        "# You are given a dataset containing sales data for different stores across various months. Each row contains the store name, the month, and the sales amount. Your task is to calculate the cumulative sales for each store, considering the monthly sales, using PySpark.\n",
        "\n",
        "# You should also:\n",
        "# Filter out stores with sales lower than 1000 in any month.\n",
        "# Calculate the total sales for each store over all months.\n",
        "# Sort the results by the total sales in descending order.\n",
        "\n",
        "# ð¬ðœð¡ðžð¦ðš ðšð§ð ððšð­ðšð¬ðžð­\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.functions import col, split, from_unixtime, sum\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"Store A\", \"2024-01\", 800),\n",
        "    (\"Store A\", \"2024-02\", 1200),\n",
        "    (\"Store A\", \"2024-03\", 900),\n",
        "    (\"Store B\", \"2024-01\", 1500),\n",
        "    (\"Store B\", \"2024-02\", 1600),\n",
        "    (\"Store B\", \"2024-03\", 1400),\n",
        "    (\"Store C\", \"2024-01\", 700),\n",
        "    (\"Store C\", \"2024-02\", 1000),\n",
        "    (\"Store C\", \"2024-03\", 800)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Store\", \"Month\", \"Sales\"])\n",
        "\n",
        "# 1\n",
        "df = df.filter(col('Sales') >= 1000)\n",
        "# 2\n",
        "window_spec = Window.partitionBy('Store').orderBy(col('Month'))\n",
        "df = df.withColumn('total_sales', sum(col('Sales')).over(window_spec))\n",
        "# 3\n",
        "df = df.sort(col('total_sales').desc())\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "IPgwx755GUzx",
        "outputId": "8b9ddbf8-ac81-4e5d-b122-fdeb4e7ece4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+-----------+\n",
            "|  Store|  Month|Sales|total_sales|\n",
            "+-------+-------+-----+-----------+\n",
            "|Store B|2024-03| 1400|       4500|\n",
            "|Store B|2024-02| 1600|       3100|\n",
            "|Store B|2024-01| 1500|       1500|\n",
            "|Store A|2024-02| 1200|       1200|\n",
            "|Store C|2024-02| 1000|       1000|\n",
            "+-------+-------+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðð®ðžð¬ð­ð¢ð¨ð§\n",
        "# You are given a dataset of stock prices with the following columns:\n",
        "\n",
        "# - stock_id: Unique identifier for the stock.\n",
        "# - date: The date of the stock price.\n",
        "# - price: The price of the stock on the given date.\n",
        "\n",
        "# Your task is to calculate the 3-day rolling average of the stock price (rolling_avg) for each stock (stock_id) using a sliding window,\n",
        "# ensuring the results are partitioned by stock_id and ordered by date.\n",
        "\n",
        "# ð¬ðœð¡ðžð¦ðš\n",
        "\n",
        "from pyspark.sql.functions import avg, col, format_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "data = [\n",
        "    (\"A\", \"2023-01-01\", 100), (\"A\", \"2023-01-02\", 105),\n",
        "    (\"A\", \"2023-01-03\", 110), (\"A\", \"2023-01-04\", 120),\n",
        "    (\"B\", \"2023-01-01\", 50), (\"B\", \"2023-01-02\", 55),\n",
        "    (\"B\", \"2023-01-03\", 60), (\"B\", \"2023-01-04\", 65),\n",
        "]\n",
        "\n",
        "schema = [\"stock_id\", \"date\", \"price\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "window_spec = Window.partitionBy('Stock_id').orderBy('date').rowsBetween(-2, 0)\n",
        "df = df.withColumn('rolling_avg', avg(col('price')).over(window_spec))\n",
        "# rolling_avg to be upto 2 decimal\n",
        "df = df.withColumn('rolling_avg', format_number(col('rolling_avg'), 2))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "DW1inDloGwfW",
        "outputId": "258a6bf8-2286-4877-aa18-3a1657455d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+-----+-----------+\n",
            "|stock_id|      date|price|rolling_avg|\n",
            "+--------+----------+-----+-----------+\n",
            "|       A|2023-01-01|  100|     100.00|\n",
            "|       A|2023-01-02|  105|     102.50|\n",
            "|       A|2023-01-03|  110|     105.00|\n",
            "|       A|2023-01-04|  120|     111.67|\n",
            "|       B|2023-01-01|   50|      50.00|\n",
            "|       B|2023-01-02|   55|      52.50|\n",
            "|       B|2023-01-03|   60|      55.00|\n",
            "|       B|2023-01-04|   65|      60.00|\n",
            "+--------+----------+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "response = requests.get(\"https://jsonplaceholder.typicode.com/users\")\n",
        "data = response.json()\n",
        "\n",
        "for i in data:\n",
        "  print(i['name'],' : ',i['email'])"
      ],
      "metadata": {
        "id": "V4bZZPvIhsyT",
        "outputId": "44ea0730-197a-4aba-8d3a-ece6e925e9a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leanne Graham  :  Sincere@april.biz\n",
            "Ervin Howell  :  Shanna@melissa.tv\n",
            "Clementine Bauch  :  Nathan@yesenia.net\n",
            "Patricia Lebsack  :  Julianne.OConner@kory.org\n",
            "Chelsey Dietrich  :  Lucio_Hettinger@annie.ca\n",
            "Mrs. Dennis Schulist  :  Karley_Dach@jasper.info\n",
            "Kurtis Weissnat  :  Telly.Hoeger@billy.biz\n",
            "Nicholas Runolfsdottir V  :  Sherwood@rosamond.me\n",
            "Glenna Reichert  :  Chaim_McDermott@dana.io\n",
            "Clementina DuBuque  :  Rey.Padberg@karina.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# Steps:\n",
        "\n",
        "def fetch_and_clean_users(api_url: str) -> pd.DataFrame:\n",
        "\n",
        "  # Fetch data from API:\n",
        "  response = requests.get(api_url)\n",
        "  data = response.json()\n",
        "\n",
        "  # Clean it:\n",
        "  # Extract name, email, and address.city\n",
        "  cleaned_data = []\n",
        "  for i in data:\n",
        "    cleaned_data.append({\n",
        "        'name': i['name'],\n",
        "        'email': i['email'],\n",
        "        'city': i['address']['city']\n",
        "    })\n",
        "\n",
        "  # Normalize names (title case), validate emails using regex\n",
        "  for i in cleaned_data:\n",
        "    i['name'] = i['name'].title()\n",
        "    if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', i['email']):\n",
        "      raise ValueError(f\"Invalid email address: {i['email']}\")\n",
        "\n",
        "  # Transform to DataFrame:\n",
        "  # Use pandas, handle missing values (simulate some), drop duplicates\n",
        "  df = pd.DataFrame(cleaned_data)\n",
        "  df = df.dropna()\n",
        "  df = df.drop_duplicates()\n",
        "\n",
        "\n",
        "  # Save to CSV locally:\n",
        "  # Call it cleaned_users.csv\n",
        "  pd.DataFrame.to_csv(df, 'cleaned_users.csv')\n",
        "  return df\n",
        "\n",
        "# Bonus:\n",
        "# Wrap your code in a function with try-except\n",
        "# Add CLI support using argparse or sys.argv\n",
        "try:\n",
        "  url = \"https://jsonplaceholder.typicode.com/users\"\n",
        "  fetch_and_clean_users(url)\n",
        "except Exception as e:\n",
        "  print('Error :', e)\n",
        "\n",
        "\n",
        "\n",
        "import unittest\n",
        "import pandas as pd\n",
        "\n",
        "class TestUserDataPipeline(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.api_url = \"https://jsonplaceholder.typicode.com/users\"\n",
        "        self.df = fetch_and_clean_users(self.api_url)\n",
        "\n",
        "    def test_columns_exist(self):\n",
        "        \"\"\"Ensure required columns are present\"\"\"\n",
        "        expected_columns = {'name', 'email', 'city'}\n",
        "        self.assertTrue(expected_columns.issubset(set(self.df.columns)))\n",
        "\n",
        "    def test_email_format(self):\n",
        "        \"\"\"Check email validity using regex\"\"\"\n",
        "        import re\n",
        "        pattern = re.compile(r\"[^@]+@[^@]+\\.[^@]+\")\n",
        "        invalid_emails = self.df[~self.df['email'].apply(lambda x: bool(pattern.match(x)))]\n",
        "        self.assertTrue(invalid_emails.empty, \"Found invalid email formats\")\n",
        "\n",
        "    def test_name_title_case(self):\n",
        "        \"\"\"Check all names are in title case\"\"\"\n",
        "        self.assertTrue(all(name == name.title() for name in self.df['name']))\n",
        "\n",
        "    def test_city_not_null(self):\n",
        "        \"\"\"Ensure no null cities\"\"\"\n",
        "        self.assertFalse(self.df['city'].isnull().any())\n",
        "\n",
        "    def test_dataframe_not_empty(self):\n",
        "        \"\"\"Ensure DataFrame has at least one row\"\"\"\n",
        "        self.assertGreater(len(self.df), 0)\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n",
        "\n",
        "import re\n",
        "import pytest\n",
        "import pandas as pd\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def cleaned_df():\n",
        "    api_url = \"https://jsonplaceholder.typicode.com/users\"\n",
        "    return fetch_and_clean_users(api_url)\n",
        "\n",
        "def test_columns_exist(cleaned_df):\n",
        "    expected_columns = {'name', 'email', 'city'}\n",
        "    assert expected_columns.issubset(set(cleaned_df.columns)), \"Missing required columns\"\n",
        "\n",
        "def test_email_format(cleaned_df):\n",
        "    pattern = re.compile(r\"[^@]+@[^@]+\\.[^@]+\")\n",
        "    invalid = cleaned_df[~cleaned_df['email'].apply(lambda x: bool(pattern.match(x)))]\n",
        "    assert invalid.empty, f\"Invalid emails found: {invalid['email'].tolist()}\"\n",
        "\n",
        "def test_name_title_case(cleaned_df):\n",
        "    non_title_names = [name for name in cleaned_df['name'] if name != name.title()]\n",
        "    assert not non_title_names, f\"Names not in title case: {non_title_names}\"\n",
        "\n",
        "def test_city_not_null(cleaned_df):\n",
        "    assert not cleaned_df['city'].isnull().any(), \"Null values found in 'city' column\"\n",
        "\n",
        "def test_dataframe_not_empty(cleaned_df):\n",
        "    assert len(cleaned_df) > 0, \"DataFrame is empty\"\n",
        "\n",
        "pytest.main()\n"
      ],
      "metadata": {
        "id": "bO21qpD1rBRV",
        "outputId": "3548d9de-b2cf-4b0b-d751-5a5df509dad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_city_not_null (__main__.TestUserDataPipeline.test_city_not_null)\n",
            "Ensure no null cities ... ok\n",
            "test_columns_exist (__main__.TestUserDataPipeline.test_columns_exist)\n",
            "Ensure required columns are present ... ok\n",
            "test_dataframe_not_empty (__main__.TestUserDataPipeline.test_dataframe_not_empty)\n",
            "Ensure DataFrame has at least one row ... ok\n",
            "test_email_format (__main__.TestUserDataPipeline.test_email_format)\n",
            "Check email validity using regex ... ok\n",
            "test_name_title_case (__main__.TestUserDataPipeline.test_name_title_case)\n",
            "Check all names are in title case ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.319s\n",
            "\n",
            "OK\n",
            "ERROR: usage: colab_kernel_launcher.py [options] [file_or_dir] [file_or_dir] [...]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n",
            "  inifile: None\n",
            "  rootdir: /root/.local/share/jupyter/runtime\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.USAGE_ERROR: 4>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEJZ2J0bvsOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}